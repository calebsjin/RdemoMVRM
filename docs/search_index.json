[
["index.html", "R code demo for fast computing algorithm 1 R code demo for fast computing algorithm 1.1 Review of Appendix B.1 Calculation 1.2 Simulation example 1.3 Time cost comparison for proposed method and for loop method", " R code demo for fast computing algorithm Shiqiang Jin and Gyuhyeong Goh 2020-06-22 1 R code demo for fast computing algorithm This is a supplymentary material about an R domo code for computing marginal likelihood distribution using fasting computing strategy and for-loop method 1.1 Review of Appendix B.1 Calculation For any \\(i\\notin\\hat{\\boldsymbol \\gamma}\\), \\(|\\hat{\\boldsymbol \\gamma}\\cup\\{i\\}|=k+1\\), hence \\(s({\\bf Y}|\\hat{\\boldsymbol \\gamma}\\cup\\{i\\})\\) in Eq.(3) can be expressed as \\[\\begin{eqnarray} s({\\bf Y}|\\hat{\\boldsymbol \\gamma}\\cup\\{i\\})={\\zeta}^{-\\frac{m(k+1)}{2}}|{\\bf X}_{\\hat{\\boldsymbol \\gamma}\\cup\\{i\\}}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf X}_{\\hat{\\boldsymbol \\gamma}\\cup\\{i\\}}+\\zeta^{-1}{\\bf I}_{k+1}|^{-\\frac{m}{2}}| {\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{\\boldsymbol \\gamma}\\cup\\{i\\}}{\\bf Y}+{\\boldsymbol \\Psi}|^{-\\frac{n+\\nu}{2}}. \\tag{1.1} \\end{eqnarray}\\] Using technique fast computing algorithm, we have \\[\\begin{eqnarray} {\\bf s}_+(\\hat{\\boldsymbol \\gamma}) &amp;=&amp; c_{\\hat{\\boldsymbol \\gamma}}^+\\times \\left(\\zeta^{-1}{\\boldsymbol 1}_p+{\\rm diag}({\\bf X}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X})\\right)^{-m/2}\\boldsymbol{\\cdot}\\\\ &amp;&amp;\\left[{\\boldsymbol 1}_p - \\frac{{\\rm diag}({\\bf X}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf Y}({\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf Y}+{\\boldsymbol \\Psi})^{-1}{\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X})}{\\zeta^{-1}{\\boldsymbol 1}_p+{\\rm diag}({\\bf X}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X})}\\right]^{-\\frac{n+\\nu}{2}}\\tag{1.2} \\end{eqnarray}\\] where \\({\\bf a}^x = (a_1^x,\\ldots,a_p^x)\\), \\({\\bf a}\\boldsymbol{\\cdot}{\\bf b}= (a_1b_1,\\ldots,a_pb_p)\\), \\({\\bf a}/{\\bf b}= (a_1/b_1,\\ldots,a_p/b_p)\\) for generic vectors \\({\\bf a}\\) and \\({\\bf b}\\), and \\(c_{\\hat{\\boldsymbol \\gamma}}^+= {\\zeta}^{-\\frac{m(k+1)}{2}}|{\\bf X}_{\\hat{\\boldsymbol \\gamma}}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf X}_{\\hat{\\boldsymbol \\gamma}}+\\zeta^{-1}{\\bf I}_{k}|^{-\\frac{m}{2}}\\left|{\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf Y}+{\\boldsymbol \\Psi}\\right|^{-\\frac{n+\\nu}{2}}\\) is a constant with respect to \\(i\\notin \\hat{\\boldsymbol \\gamma}\\). Hence, \\[\\begin{eqnarray} \\log({\\bf s}_+(\\hat{\\boldsymbol \\gamma})) = \\log(c_{\\hat{\\boldsymbol \\gamma}}^+){\\boldsymbol 1}_p-\\frac{m}{2}\\log({\\bf d})-\\frac{n+\\nu}{2}\\log(1-\\frac{{\\bf u}}{{\\bf d}}),\\tag{1.3} \\end{eqnarray}\\] where \\({\\bf d}= \\zeta^{-1}{\\boldsymbol 1}_p+{\\rm diag}({\\bf X}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X})\\) and \\({\\bf u}={\\rm diag}({\\bf X}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf Y}({\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf Y}+{\\boldsymbol \\Psi})^{-1}{\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X})\\). In the following simulation example, I will evaluate \\({\\bf s}({\\bf Y}|{\\text{nbd}}_+(\\hat{\\boldsymbol \\gamma}))\\) by (1.1) in the “for-loop” method and by (1.3) in a single calculation. 1.2 Simulation example In this example, we specify the model setting as \\(n=100, p = 1000, m = 5, \\zeta = \\log(n), {\\boldsymbol \\Psi}= 0.5{\\bf I}_m, \\nu = 0.5\\). n &lt;- 100 p &lt;- 1000 m &lt;- 5 zeta &lt;- log(n) Psi &lt;- diag(0.5, m) # Psi v &lt;- 0.5 # nu and generate data \\({\\bf Y}={\\bf X}{\\bf C}+ {\\bf E}\\) with \\({\\bf E}\\sim \\mathcal{N}(0, {\\boldsymbol\\Omega})\\); The true model is \\({\\boldsymbol \\gamma}^* = (1,2,3,4,7,8,9,10)\\) and the current model \\(\\hat{\\boldsymbol \\gamma}= (1,2,3,4,7,8,9)\\) with model size \\(|\\hat{\\boldsymbol \\gamma}| = 7\\). \\({\\boldsymbol\\Omega}= 0.2^{|i-j|}\\) and \\({\\bf X}\\) is generated from \\(\\mathcal{N}({\\bf 0}, {\\boldsymbol \\Sigma})\\) with \\({\\boldsymbol \\Sigma}= 0.2^{|i-j|}\\). # Generate data library(mvtnorm) set.seed(1314) true.model &lt;- c(1:4, 7:10) # true model r &lt;- c(1:4, 7:9) # current model k &lt;- length(r) # current model size rho_e &lt;- 0.2 Omega &lt;- rho_e^(abs(matrix(1:m, m, m) - t(matrix(1:m, m, m)))) rho_x &lt;- 0.2 Sig_x &lt;- rho_x^(abs(matrix(1:p, p, p) - t(matrix(1:p, p, p)))) seq.p &lt;- c(1:p) len.true.model &lt;- length(true.model) # generate random coefficient matrix C c0 &lt;- sample(seq(-1, 1, 0.2), size = len.true.model * m, replace = TRUE) C &lt;- matrix(0, p, m) C[true.model, ] &lt;- matrix(c0, len.true.model, m) X &lt;- rmvnorm(n, rep(0, p), Sig_x, method = &quot;chol&quot;) E &lt;- rmvnorm(n, mean = rep(0, m), sigma = Omega) Y &lt;- as.numeric(X %*% C) + E To better understand R code and corresponding notations, we list a cross-reference table for some of them as follows: I_n I_k1 log.s.plus1 or log.s.plus2 rUi X.rUi H.rUi \\({\\bf I}_n\\) \\({\\bf I}_{k+1}\\) \\(\\log({\\bf s}({\\bf Y}|{\\text{nbd}}_+(\\hat{\\boldsymbol \\gamma})))\\) \\(\\hat{\\boldsymbol \\gamma}\\cup i\\) \\({\\bf X}_{\\hat{\\boldsymbol \\gamma}\\cup i}\\) \\({\\bf H}_{\\hat{\\boldsymbol \\gamma}\\cup i}\\) log.s.Y.rUi I_k X.r X_r H.r colSums(H.r%*%X_r*X_r) \\(\\log(s({\\bf Y}|\\hat{\\boldsymbol \\gamma}\\cup i))\\) \\({\\bf I}_k\\) \\({\\bf X}_{\\hat{\\boldsymbol \\gamma}}\\) \\({\\bf X}_{-\\hat{\\boldsymbol \\gamma}}\\) \\({\\bf H}_{\\hat{\\boldsymbol \\gamma}}\\) \\({\\rm diag}({\\bf X}_{-\\hat{\\boldsymbol \\gamma}}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X}_{-\\hat{\\boldsymbol \\gamma}})\\) YHX_r \\({\\bf Y}^{{ \\mathrm{\\scriptscriptstyle T} }}{\\bf H}_{\\hat{{\\boldsymbol \\gamma}}}{\\bf X}_{-\\hat{\\boldsymbol \\gamma}}\\) # For loop method I_n &lt;- diag(1, n) # n-dimension identity matrix I_k1 &lt;- diag(1, k + 1) p_r &lt;- setdiff(seq(1, p), r) # p-k vector log.s.plus1 &lt;- rep(NA, length(p_r)) j &lt;- 1 for (i in p_r) { rUi &lt;- sort(c(r, i)) # add one index from p_r X.rUi &lt;- X[, rUi] # model in addition neighbor XtX &lt;- crossprod(X.rUi) + 1/zeta * I_k1 H.rUi &lt;- I_n - X.rUi %*% solve(XtX) %*% t(X.rUi) # logarithm of Eq (1.1) log.s.Y.rUi &lt;- -m * (k + 1)/2 * log(zeta) - m/2 * log(det(XtX)) - (n + v)/2 * log(det(t(Y) %*% H.rUi %*% Y + Psi)) log.s.plus1[j] &lt;- log.s.Y.rUi j &lt;- j + 1 } # Proposed Method I_k &lt;- diag(1, k) # k-dimension identity matrix X.r &lt;- X[, r] X_r &lt;- X[, p_r] # n by p-k m sub-matrix of X H.r &lt;- I_n - X.r %*% solve(crossprod(X.r) + 1/zeta * I_k) %*% t(X.r) # n by n matrix d &lt;- 1/zeta + colSums(H.r %*% X_r * X_r) # p-k dimension vector YHX_r &lt;- t(Y) %*% H.r %*% X_r # p-k by m matrix YHY_1 &lt;- solve(t(Y) %*% H.r %*% Y + Psi) # m by m matrix u &lt;- colSums(YHY_1 %*% YHX_r * YHX_r) # p-k dimension vector # logarithm of Eq (1.3) log.s.plus1.approx &lt;- -m/2 * log(d) - (n + v)/2 * log(1 - u/d) # log(c) log.c &lt;- -0.5 * m * (k + 1) * log(zeta) - 0.5 * m * log(det(crossprod(X.r) + 1/zeta * I_k)) - (n + v)/2 * log(det(t(Y) %*% H.r %*% Y + Psi)) log.s.plus2 &lt;- log.c + log.s.plus1.approx # logarithm of Eq (1.2) I compute mean absolute percentage error \\(\\text{MAPE} = \\frac{1}{n}\\Sigma_{t=1}^n|\\frac{A_t-F_t}{A_t}|\\) to measure the accuracy of the fast computing algorithm. # Mean absolute percentage error MAPE &lt;- mean(abs(log.s.plus1 - log.s.plus2)/abs(log.s.plus1)) print(paste(&quot;MAPE =&quot;, MAPE)) ## [1] &quot;MAPE = 8.12298413315687e-17&quot; plot(log.s.plus1, log.s.plus2) abline(a = 0, b = 1) From the plot and MAPE, \\(\\log({\\bf s}({\\bf Y}|{\\text{nbd}}_+(\\hat{\\boldsymbol \\gamma})))\\) computed by (1.1) and (1.3) are the same. But the time costs are different. 1.3 Time cost comparison for proposed method and for loop method Note that when doing the model selection, as \\(\\log(c_{\\hat{\\boldsymbol \\gamma}}^+)\\) is a constant with respect to \\(i\\notin \\hat{\\boldsymbol \\gamma}\\), in R code I only compute log.s.plus2.approx. I use R package microbenchmark to do the simulation and the default replication is 100 times. library(microbenchmark) timecost &lt;- microbenchmark(&quot;for_loop&quot; = { log.s.plus1 &lt;- rep(NA, length(p_r)) j &lt;- 1 for (i in p_r) { rUi &lt;- sort(c(r, i)) # add one index from p_r X.rUi &lt;- X[, rUi] # model in addition neighbor XtX &lt;- crossprod(X.rUi) + 1/zeta * I_k1 H.rUi &lt;- I_n - X.rUi %*% solve(XtX) %*% t(X.rUi) # logarithm of Eq (1.1) log.s.Y.rUi &lt;- -m * (k + 1)/2 * log(zeta) - m/2 * log(det(XtX)) - (n + v)/2 * log(det(t(Y) %*% H.rUi %*% Y + Psi)) log.s.plus1[j] &lt;- log.s.Y.rUi j &lt;- j + 1 } }, &quot;Proposed&quot; = { X.r &lt;- X[, r] I_k &lt;- diag(1, k) # k-dimension identity matrix X_r &lt;- X[, p_r] # n by p-k m sub-matrix of X H.r &lt;- I_n - X.r %*% solve(crossprod(X.r) + 1/zeta * I_k) %*% t(X.r) # n by n matrix d &lt;- 1/zeta + colSums(H.r %*% X_r * X_r) # p-k dimension vector YHX &lt;- t(Y) %*% H.r %*% X_r # p-k by m matrix YHY_1 &lt;- solve(t(Y) %*% H.r %*% Y + Psi) # m by m matrix u &lt;- colSums(YHY_1 %*% YHX * YHX) # p-k dimension vector # logarithm of Eq (3) log.s.plus2.approx &lt;- -m/2 * log(d) - (n + v)/2 * log(1 - u/d) } ) timecost ## Unit: milliseconds ## expr min lq mean median uq max ## for_loop 133.158391 146.798764 179.950596 162.110519 207.067868 297.456567 ## Proposed 1.241057 1.349924 1.740952 1.441072 1.782093 4.583361 ## neval ## 100 ## 100 Looking at the median of time cost, the proposed method is about 100 times faster than the for-loop method. "]
]
