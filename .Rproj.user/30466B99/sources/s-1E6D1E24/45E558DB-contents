---
header-includes:
- \usepackage{amssymb}
- \usepackage{color}
- \documentclass{article}
- \usepackage{amsmath}
- \usepackage{setspace}
- \usepackage{xcolor}
- \usepackage{graphicx}
output:
pdf_document: newcommandault
html_document: newcommandault
---
\newcommand{\uA}{{\bf A}}
\newcommand{\ua}{{\bf a}}
\newcommand{\uB}{{\bf B}}
\newcommand{\ub}{{\bf b}}
\newcommand{\uC}{{\bf C}}
\newcommand{\uc}{{\bf c}}
\newcommand{\ud}{{\bf d}}
\newcommand{\ue}{{\bf e}}
\newcommand{\uE}{{\bf E}}
\newcommand{\uH}{{\bf H}}
\newcommand{\uI}{{\bf I}}
\newcommand{\uK}{{\bf K}}
\newcommand{\bP}{{\bf P}}
\newcommand{\uQ}{{\bf Q}}
\newcommand{\uv}{{\bf v}}
\newcommand{\uV}{{\bf V}}
\newcommand{\us}{{\bf s}}
\newcommand{\T}{{ \mathrm{\scriptscriptstyle T} }}
\newcommand{\uU}{{\bf U}}
\newcommand{\uu}{{\bf u}}
\newcommand{\uX}{{\bf X}}
\newcommand{\ux}{{\bf x}}
\newcommand{\uY}{{\bf Y}}
\newcommand{\uy}{{\bf y}}

<!-- greek letters, numbers and etc.-->

\newcommand{\0}{{\bf 0}}
\newcommand{\1}{{\boldsymbol 1}}
\newcommand{\ualpha}{{\boldsymbol \alpha}}
\newcommand{\ubeta}{{\boldsymbol \beta}}
\newcommand{\diag}{{\rm diag}}
\newcommand{\uepsilon}{{\boldsymbol \epsilon}}
\newcommand{\ueta}{{\boldsymbol \eta}}
\newcommand{\bg}{{\boldsymbol \gamma}}
\newcommand{\bOmega}{{\boldsymbol\Omega}}
\newcommand{\uPsi}{{\boldsymbol \Psi}}
\newcommand{\uSigma}{{\boldsymbol \Sigma}}
\newcommand{\uxi}{{\boldsymbol \xi}}
\newcommand{\nbd}{{\text{nbd}}}

# R code demo for Bayes MVRM

## Review of Appendix B.1 Calculation
For any $i\notin\hat\bg$, $|\hat\bg\cup\{i\}|=k+1$, hence $s(\uY|\hat\bg\cup\{i\})$ in Eq.(3) can be expressed as
\begin{eqnarray}
s(\uY|\hat\bg\cup\{i\})={\zeta}^{-\frac{m(k+1)}{2}}|\uX_{\hat\bg\cup\{i\}}^{\T}\uX_{\hat\bg\cup\{i\}}+\zeta^{-1}\uI_{k+1}|^{-\frac{m}{2}}| \uY^{\T}\uH_{\hat\bg\cup\{i\}}\uY +\uPsi|^{-\frac{n+\nu}{2}}.
(\#eq:1)
\end{eqnarray}

Using technique fast computing algorithm, we have

\begin{eqnarray}
\us_+(\hat\bg) &=& c_{\hat\bg}^+\times \left(\zeta^{-1}\1_p+\diag(\uX^{\T}\uH_{\hat{\bg}}\uX)\right)^{-m/2}\boldsymbol{\cdot}\\
&&\left[\1_p - \frac{\diag(\uX^{\T}\uH_{\hat{\bg}}\uY(\uY^{\T}\uH_{\hat{\bg}}\uY+\uPsi)^{-1}\uY^{\T}\uH_{\hat{\bg}}\uX)}{\zeta^{-1}\1_p+\diag(\uX^{\T}\uH_{\hat{\bg}}\uX)}\right]^{-\frac{n+\nu}{2}}(\#eq:2)
\end{eqnarray}
where $\ua^x = (a_1^x,\ldots,a_p^x)$, $\ua\boldsymbol{\cdot}\ub = (a_1b_1,\ldots,a_pb_p)$, $\ua/\ub = (a_1/b_1,\ldots,a_p/b_p)$ for generic vectors $\ua$ and $\ub$, and $c_{\hat\bg}^+= {\zeta}^{-\frac{m(k+1)}{2}}|\uX_{\hat\bg}^{\T}\uX_{\hat\bg}+\zeta^{-1}\uI_{k}|^{-\frac{m}{2}}\left|\uY^{\T}\uH_{\hat{\bg}}\uY+\uPsi\right|^{-\frac{n+\nu}{2}}$ is a constant with respect to $i\notin \hat\bg$.

Hence, 
\begin{eqnarray}
\log(\us_+(\hat\bg)) = \log(c_{\hat\bg}^+)\1_p-\frac{m}{2}\log(\ud)-\frac{n+\nu}{2}\log(1-\frac{\uu}{\ud}),(\#eq:3)
\end{eqnarray}
where $\ud = \zeta^{-1}\1_p+\diag(\uX^{\T}\uH_{\hat{\bg}}\uX)$ and $\uu=\diag(\uX^{\T}\uH_{\hat{\bg}}\uY(\uY^{\T}\uH_{\hat{\bg}}\uY+\uPsi)^{-1}\uY^{\T}\uH_{\hat{\bg}}\uX)$.

In the following simulation example, I will evaluate $\us(\uY|\nbd_+(\hat\bg))$ by \@ref(eq:1) in the "for-loop" method and by \@ref(eq:3) in a single calculation. 

## Simulation example

In this example, we 

- specify the model setting as $n=100, p = 1000, m = 5, \zeta = \log(n), \uPsi = 0.5\uI_m, \nu = 0.5$.

```{r}
n <- 100
p <- 1000
m <- 5
zeta <- log(n)
Psi <- diag(0.5, m)  # Psi
v <- 0.5  # nu
```

- and generate data $\uY=\uX\uC + \uE$ with $\uE\sim \mathcal{N}(0, \bOmega)$; The true model is $\bg^* = (1,2,3,4,7,8,9,10)$ and the current model $\hat\bg = (1,2,3,4,7,8,9)$ with model size $|\hat\bg| = 7$. $\bOmega = 0.2^{|i-j|}$ and $\uX$ is generated from $\mathcal{N}(\0, \uSigma)$ with $\uSigma = 0.2^{|i-j|}$.

```{r}
# Generate data
library(mvtnorm)
set.seed(1314)
true.model <- c(1:4, 7:10)  # true model
r <- c(1:4, 7:9)  # current model
k <- length(r)  # current model size
rho_e <- 0.2
Omega <- rho_e^(abs(matrix(1:m, m, m) - t(matrix(1:m, m, m))))
rho_x <- 0.2
Sig_x <- rho_x^(abs(matrix(1:p, p, p) - t(matrix(1:p, p, p))))
seq.p <- c(1:p)
len.true.model <- length(true.model)
# generate random coefficient matrix C
c0 <- sample(seq(-1, 1, 0.2), size = len.true.model * m, replace = TRUE)
C <- matrix(0, p, m)
C[true.model, ] <- matrix(c0, len.true.model, m)
X <- rmvnorm(n, rep(0, p), Sig_x, method = "chol")
E <- rmvnorm(n, mean = rep(0, m), sigma = Omega)
Y <- as.numeric(X %*% C) + E
```

To better understand R code and corresponding notations, we list a cross-reference table for some of them as follows:$(\uY^{\T}\uH_{\hat{\bg}}\uY+\uPsi)^{-1}$|

|||||||
|:--:|:--:|:--:|:--:|:--:|:--:|
|I_n| I_k1 | log.s.plus1 or log.s.plus2 | rUi | X.rUi | H.rUi |
|$\uI_n$|$\uI_{k+1}$|$\log(\us(\uY|\nbd_+(\hat\bg)))$|$\hat\bg\cup i$|$\uX_{\hat\bg\cup i}$|$\uH_{\hat\bg\cup i}$|
|log.s.Y.rUi|I_k|X.r|X_r| H.r | colSums(H.r\%\*\%X\_r\*X\_r)|
|$\log(s(\uY|\hat\bg\cup i))$|$\uI_k$|$\uX_{\hat\bg}$|$\uX_{-\hat\bg}$|$\uH_{\hat\bg}$|$\diag(\uX_{-\hat\bg}^{\T}\uH_{\hat{\bg}}\uX_{-\hat\bg})$|
|YHX_r|
|$\uY^{\T}\uH_{\hat{\bg}}\uX_{-\hat\bg}$|
```{r}
# For loop method
I_n <- diag(1, n)  # n-dimension identity matrix 
I_k1 <- diag(1, k + 1)
p_r <- setdiff(seq(1, p), r)  # p-k vector
log.s.plus1 <- rep(NA, length(p_r))
j <- 1
for (i in p_r) {
    rUi <- sort(c(r, i))  # add one index from p_r
    X.rUi <- X[, rUi]  # model in addition neighbor
    XtX <- crossprod(X.rUi) + 1/zeta * I_k1
    H.rUi <- I_n - X.rUi %*% solve(XtX) %*% t(X.rUi)
    # logarithm of Eq (1.1)
    log.s.Y.rUi <- -m * (k + 1)/2 * log(zeta) - m/2 * log(det(XtX)) - (n + v)/2 * log(det(t(Y) %*% H.rUi %*% Y + Psi))
    log.s.plus1[j] <- log.s.Y.rUi
    j <- j + 1
}

# Proposed Method
I_k <- diag(1, k)  # k-dimension identity matrix 
X.r <- X[, r]
X_r <- X[, p_r]  # n by p-k m sub-matrix of X
H.r <- I_n - X.r %*% solve(crossprod(X.r) + 1/zeta * I_k) %*% t(X.r)  # n by n matrix
d <- 1/zeta + colSums(H.r %*% X_r * X_r)  # p-k dimension vector
YHX_r <- t(Y) %*% H.r %*% X_r  # p-k by m matrix
YHY_1 <- solve(t(Y) %*% H.r %*% Y + Psi)  # m by m matrix
u <- colSums(YHY_1 %*% YHX_r * YHX_r)  # p-k dimension vector

# logarithm of Eq (1.3)
log.s.plus1.approx <- -m/2 * log(d) - (n + v)/2 * log(1 - u/d)
# log(c)
log.c <- -0.5 * m * (k + 1) * log(zeta) - 0.5 * m * log(det(crossprod(X.r) + 
    1/zeta * I_k)) - (n + v)/2 * log(det(t(Y) %*% H.r %*% Y + Psi))
log.s.plus2 <- log.c + log.s.plus1.approx  # logarithm of Eq (1.2)
```

I compute mean absolute percentage error $\text{MAPE} = \frac{1}{n}\Sigma_{t=1}^n|\frac{A_t-F_t}{A_t}|$ to measure the accuracy of the fast computing algorithm. 
```{r}
# Mean absolute percentage error
MAPE <- mean(abs(log.s.plus1 - log.s.plus2)/abs(log.s.plus1))
print(paste("MAPE =", MAPE))
plot(log.s.plus1, log.s.plus2)
abline(a = 0, b = 1)
```

From the plot and MAPE, $\log(\us(\uY|\nbd_+(\hat\bg)))$ computed by \@ref(eq:1) and \@ref(eq:3) are the same. But the time costs are different.

## Time cost comparison for proposed method and for loop method

Note that when doing the model selection, as $\log(c_{\hat\bg}^+)$ is a constant with respect to $i\notin \hat\bg$, in R code I only compute `log.s.plus2.approx`. I use R package `microbenchmark` to do the simulation and the default replication is 100 times. 
```{r, eval= FALSE}
library(microbenchmark)
timecost <- microbenchmark("for_loop" = {
  log.s.plus1 <- rep(NA, length(p_r))
  j <- 1
  for (i in p_r) {
    rUi <- sort(c(r, i))  # add one index from p_r
    X.rUi <- X[, rUi]  # model in addition neighbor
    XtX <- crossprod(X.rUi) + 1/zeta * I_k1
    H.rUi <- I_n - X.rUi %*% solve(XtX) %*% t(X.rUi)
    # logarithm of Eq (1.1)
    log.s.Y.rUi <- -m * (k + 1)/2 * log(zeta) - 
      m/2 * log(det(XtX)) - (n + v)/2 * log(det(t(Y) %*% H.rUi %*% Y + Psi))
    log.s.plus1[j] <- log.s.Y.rUi
    j <- j + 1
  }
},
"Proposed" = {
  X.r <- X[, r]
  I_k <- diag(1, k)  # k-dimension identity matrix 
  X_r <- X[, p_r]  # n by p-k m sub-matrix of X
  H.r <- I_n - X.r %*% solve(crossprod(X.r) + 1/zeta * I_k) %*% t(X.r)  # n by n matrix
  d <- 1/zeta + colSums(H.r %*% X_r * X_r)  # p-k dimension vector
  YHX <- t(Y) %*% H.r %*% X_r  # p-k by m matrix
  YHY_1 <- solve(t(Y) %*% H.r %*% Y + Psi)  # m by m matrix
  u <- colSums(YHY_1 %*% YHX * YHX)  # p-k dimension vector
  # logarithm of Eq (3)
  log.s.plus2.approx <- -m/2 * log(d) - (n + v)/2 * log(1 - u/d)
}
) 
timecost
```
Looking at the median of time cost, the proposed method is about 100 times faster than the for-loop method. 